{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b82424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Model BERT Spam Detection Indonesia\n",
    "# Fine-tune ulang dari model nahiar/spam-detection-bert-v1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d59129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Model dan Tokenizer yang sudah ada\n",
    "print(\"Loading model dan tokenizer...\")\n",
    "\n",
    "MODEL_NAME = \"nahiar/spam-detection-bert-v1\"\n",
    "\n",
    "# Load tokenizer dan model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Max length: {tokenizer.model_max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52478cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load dan Prepare Dataset\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Load combined dataset (atau gunakan dataset baru jika ada)\n",
    "df = pd.read_csv('../data/combined_dataset.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nDistribusi label:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Sampling dataset jika terlalu besar (opsional)\n",
    "# df = df.sample(n=5000, random_state=42)  # Uncomment jika perlu sampling\n",
    "\n",
    "# Clean data\n",
    "df = df.dropna()\n",
    "df['text'] = df['text'].str.strip()\n",
    "df = df[df['text'].str.len() > 0]\n",
    "\n",
    "print(f\"\\nAfter cleaning - Dataset shape: {df.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tokenization dan Dataset Preparation\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text data untuk training\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Split dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].tolist(),\n",
    "    df['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': train_texts,\n",
    "    'labels': train_labels\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'text': val_texts,\n",
    "    'labels': val_labels\n",
    "})\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Dataset tokenization completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Metric Computation Function\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics untuk evaluasi model\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'eval_loss': 0.0  # Will be computed automatically\n",
    "    }\n",
    "\n",
    "# 5. Training Arguments\n",
    "print(\"Setting up training arguments...\")\n",
    "\n",
    "OUTPUT_DIR = \"../models/v2\"  # Directory untuk save model baru\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,  # Bisa disesuaikan, untuk fine-tune ulang biasanya 1-3 epoch cukup\n",
    "    per_device_train_batch_size=16,  # Sesuaikan dengan GPU memory\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'{OUTPUT_DIR}/logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None,  # Disable wandb/tensorboard\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    dataloader_num_workers=4,\n",
    "    save_total_limit=2,  # Keep only 2 best checkpoints\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2eeb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Setup Trainer\n",
    "print(\"Setting up trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"Trainer configured!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087af399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Start Fine-tuning\n",
    "print(\"=\" * 50)\n",
    "print(\"MEMULAI FINE-TUNING MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Evaluate model sebelum fine-tuning\n",
    "print(\"Evaluating model sebelum fine-tuning...\")\n",
    "eval_results_before = trainer.evaluate()\n",
    "print(f\"Accuracy sebelum fine-tuning: {eval_results_before['eval_accuracy']:.4f}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nMemulai training...\")\n",
    "training_start_time = pd.Timestamp.now()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "training_end_time = pd.Timestamp.now()\n",
    "training_duration = training_end_time - training_start_time\n",
    "\n",
    "print(f\"\\nTraining selesai!\")\n",
    "print(f\"Durasi training: {training_duration}\")\n",
    "\n",
    "# Evaluate model setelah fine-tuning\n",
    "print(\"\\nEvaluating model setelah fine-tuning...\")\n",
    "eval_results_after = trainer.evaluate()\n",
    "print(f\"Accuracy setelah fine-tuning: {eval_results_after['eval_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nImprovement: {eval_results_after['eval_accuracy'] - eval_results_before['eval_accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Save Model\n",
    "print(\"Saving fine-tuned model...\")\n",
    "\n",
    "# Save model dan tokenizer\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model berhasil disimpan di: {OUTPUT_DIR}\")\n",
    "\n",
    "# Create model info file\n",
    "model_info = {\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"fine_tuned_on\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"training_samples\": len(train_texts),\n",
    "    \"validation_samples\": len(val_texts),\n",
    "    \"accuracy_before\": eval_results_before['eval_accuracy'],\n",
    "    \"accuracy_after\": eval_results_after['eval_accuracy'],\n",
    "    \"improvement\": eval_results_after['eval_accuracy'] - eval_results_before['eval_accuracy'],\n",
    "    \"training_duration\": str(training_duration)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"{OUTPUT_DIR}/model_info.json\", \"w\") as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"Model info saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Test Model Baru\n",
    "print(\"=\" * 50)\n",
    "print(\"TESTING MODEL YANG SUDAH DI-FINE-TUNE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test dengan beberapa contoh text\n",
    "test_texts = [\n",
    "    \"Dapatkan hadiah gratis dengan klik link ini!\",\n",
    "    \"Selamat pagi, bagaimana kabarnya hari ini?\",\n",
    "    \"GRATIS! Klik link ini untuk mendapat hadiah jutaan rupiah!\",\n",
    "    \"Terima kasih atas informasinya, sangat membantu\",\n",
    "    \"URGENT! Transfer uang sekarang juga untuk mendapat bonus\",\n",
    "    \"Besok ada rapat penting di kantor\",\n",
    "    \"Klik www.gratishadiah.com untuk mendapat iPhone gratis!\",\n",
    "    \"Selamat hari raya, semoga bahagia selalu\"\n",
    "]\n",
    "\n",
    "print(\"Testing model dengan sample texts:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "\n",
    "    label = \"SPAM\" if predicted_class == 1 else \"HAM\"\n",
    "\n",
    "    print(f\"{i}. Text: {text}\")\n",
    "    print(f\"   Prediction: {label} (confidence: {confidence:.4f})\")\n",
    "    print()\n",
    "\n",
    "print(\"Testing selesai!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c11413",
   "metadata": {},
   "source": [
    "# Cara Menggunakan Model yang Sudah Di-Fine-tune\n",
    "\n",
    "Setelah fine-tuning selesai, Anda bisa menggunakan model baru dengan cara berikut:\n",
    "\n",
    "## 1. Load Model Lokal\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load model dari direktori lokal\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../models/v2\")\n",
    "```\n",
    "\n",
    "## 2. Menggunakan Pipeline\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Buat classifier pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"../models/v2\",\n",
    "    tokenizer=\"../models/v2\"\n",
    ")\n",
    "\n",
    "# Test\n",
    "result = classifier(\"Dapatkan hadiah gratis sekarang!\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "## 3. Upload ke Hugging Face Hub (Opsional)\n",
    "\n",
    "Jika ingin mengupload model baru ke Hugging Face:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Login ke Hugging Face (perlu token)\n",
    "# huggingface-cli login\n",
    "\n",
    "# Upload model\n",
    "model.push_to_hub(\"nahiar/spam-detection-bert-v2\")\n",
    "tokenizer.push_to_hub(\"nahiar/spam-detection-bert-v2\")\n",
    "```\n",
    "\n",
    "## Tips Fine-tuning:\n",
    "\n",
    "1. **Epoch**: Untuk fine-tune ulang, biasanya 1-3 epoch sudah cukup\n",
    "2. **Learning Rate**: Gunakan learning rate yang lebih kecil (default biasanya ok)\n",
    "3. **Batch Size**: Sesuaikan dengan GPU memory yang tersedia\n",
    "4. **Data**: Pastikan data bersih dan seimbang antara spam/ham\n",
    "5. **Evaluasi**: Selalu evaluasi model sebelum dan sesudah fine-tuning\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
