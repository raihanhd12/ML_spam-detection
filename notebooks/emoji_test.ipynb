{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Emoji Support in Current Tokenizer\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "\n",
        "console = Console()\n",
        "\n",
        "# Load tokenizer dan model saat ini\n",
        "model_dir = \"/Users/rhd/Documents/Raihan/Dev/Model-ML/spam-detection-twitter/models/v2\"\n",
        "\n",
        "# Current tokenizer - IndoBERTweet (sudah bagus untuk social media + emoji)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "model.eval()\n",
        "\n",
        "print(\"ü§ñ Model dan Tokenizer loaded!\")\n",
        "print(f\"üìù Tokenizer type: {type(tokenizer).__name__}\")\n",
        "print(f\"üß† Model type: {type(model).__name__}\")\n",
        "print(f\"üìñ Vocab size: {tokenizer.vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test berbagai text dengan emoji\n",
        "test_texts = [\n",
        "    # Text tanpa emoji\n",
        "    \"Beli produk murah disini sekarang juga\",\n",
        "\n",
        "    # Text dengan emoji sederhana\n",
        "    \"Halo üòä apa kabar?\",\n",
        "    \"Spam message üö®üî• klik link ini sekarang!!!\",\n",
        "\n",
        "    # Text dengan emoji kompleks\n",
        "    \"PROMO GILA üéâüéäüí• Diskon 90% hari ini saja! üí∞üí∏ Jangan sampai terlewat! ‚è∞üèÉ‚Äç‚ôÇÔ∏è\",\n",
        "    \"Selamat pagi teman-teman üåÖ‚òÄÔ∏è semoga hari ini menyenangkan üíñ\",\n",
        "\n",
        "    # Text dengan emoji yang sering dipakai spam\n",
        "    \"üî•üî•üî• URGENT!!! Transfer sekarang juga üí∞üí∞üí∞\",\n",
        "    \"‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è Love you baby üíãüíãüíã\",\n",
        "\n",
        "    # Text dengan emoji Indonesia\n",
        "    \"Lagi hujan nih ‚òî di Jakarta üèôÔ∏è\",\n",
        "    \"Makan nasi gudeg üçõ enak banget! üòãüëç\"\n",
        "]\n",
        "\n",
        "def test_emoji_tokenization(text):\n",
        "    \"\"\"Test bagaimana tokenizer handle emoji\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üìù Original: {text}\")\n",
        "\n",
        "    # Tokenize\n",
        "    encoded = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded)\n",
        "        probabilities = torch.softmax(outputs.logits, dim=-1)\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "        confidence = probabilities[0][prediction].item()\n",
        "\n",
        "    label = \"SPAM\" if prediction == 1 else \"HAM\"\n",
        "    emoji_result = \"üö®\" if label == \"SPAM\" else \"‚úÖ\"\n",
        "\n",
        "    print(f\"üéØ Prediction: {emoji_result} {label} (confidence: {confidence:.3f})\")\n",
        "    print(f\"üî§ Tokens ({len(tokens)}): {tokens}\")\n",
        "\n",
        "    # Hitung berapa emoji yang di-tokenize\n",
        "    emoji_tokens = [t for t in tokens if any(ord(char) > 127 for char in t if char not in ['[', ']'])]\n",
        "    print(f\"üòÄ Emoji tokens detected: {len(emoji_tokens)} - {emoji_tokens}\")\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'prediction': label,\n",
        "        'confidence': confidence,\n",
        "        'tokens': tokens,\n",
        "        'emoji_tokens': emoji_tokens\n",
        "    }\n",
        "\n",
        "# Test semua text\n",
        "results = []\n",
        "for text in test_texts:\n",
        "    result = test_emoji_tokenization(text)\n",
        "    results.append(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisis hasil dan rekomendasi\n",
        "import pandas as pd\n",
        "\n",
        "# Buat summary table\n",
        "summary_df = pd.DataFrame([\n",
        "    {\n",
        "        'Text': r['text'][:50] + \"...\" if len(r['text']) > 50 else r['text'],\n",
        "        'Prediction': r['prediction'],\n",
        "        'Confidence': f\"{r['confidence']:.3f}\",\n",
        "        'Total_Tokens': len(r['tokens']),\n",
        "        'Emoji_Tokens': len(r['emoji_tokens']),\n",
        "        'Has_Emoji': 'Yes' if r['emoji_tokens'] else 'No'\n",
        "    }\n",
        "    for r in results\n",
        "])\n",
        "\n",
        "print(\"üìä SUMMARY HASIL TEST EMOJI:\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Analisis emoji support\n",
        "emoji_texts = [r for r in results if r['emoji_tokens']]\n",
        "non_emoji_texts = [r for r in results if not r['emoji_tokens']]\n",
        "\n",
        "print(f\"\\nüîç ANALISIS:\")\n",
        "print(f\"‚úÖ Text dengan emoji: {len(emoji_texts)}/{len(results)}\")\n",
        "print(f\"üìù Text tanpa emoji: {len(non_emoji_texts)}/{len(results)}\")\n",
        "\n",
        "if emoji_texts:\n",
        "    avg_emoji_confidence = sum(r['confidence'] for r in emoji_texts) / len(emoji_texts)\n",
        "    print(f\"üéØ Rata-rata confidence untuk text dengan emoji: {avg_emoji_confidence:.3f}\")\n",
        "\n",
        "    # Cek apakah emoji di-tokenize dengan baik\n",
        "    emoji_detection_good = sum(1 for r in emoji_texts if r['emoji_tokens']) / len(emoji_texts)\n",
        "    print(f\"üòÄ Emoji detection rate: {emoji_detection_good:.2%}\")\n",
        "\n",
        "print(f\"\\nüí° REKOMENDASI:\")\n",
        "print(\"1. ‚úÖ IndoBERTweet tokenizer SUDAH support emoji dengan baik\")\n",
        "print(\"2. üéØ Model bisa detect pattern spam dengan emoji\")\n",
        "print(\"3. üìö Tapi untuk improve accuracy, perlu:\")\n",
        "print(\"   - Tambah data training dengan lebih banyak emoji\")\n",
        "print(\"   - Fine-tune dengan dataset yang kaya emoji\")\n",
        "print(\"   - Tambah preprocessing khusus emoji jika perlu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CARA IMPROVE EMOJI DETECTION - Data Augmentation\n",
        "import random\n",
        "\n",
        "def augment_text_with_emoji(text, label):\n",
        "    \"\"\"Tambahkan emoji ke text berdasarkan label untuk data augmentation\"\"\"\n",
        "\n",
        "    # Emoji untuk spam messages\n",
        "    spam_emojis = [\"üî•\", \"üí∞\", \"üí∏\", \"üö®\", \"‚ö†Ô∏è\", \"‚ùó\", \"‚ÄºÔ∏è\", \"üí•\", \"üéâ\", \"üéä\", \"‚è∞\", \"üèÉ‚Äç‚ôÇÔ∏è\", \"üèÉ‚Äç‚ôÄÔ∏è\"]\n",
        "\n",
        "    # Emoji untuk ham messages\n",
        "    ham_emojis = [\"üòä\", \"üòÑ\", \"üëç\", \"‚ù§Ô∏è\", \"üíñ\", \"üåÖ\", \"‚òÄÔ∏è\", \"‚òî\", \"üèôÔ∏è\", \"üçõ\", \"üòã\", \"üôè\"]\n",
        "\n",
        "    augmented_texts = []\n",
        "\n",
        "    if label == 'spam':\n",
        "        # Untuk spam, tambah emoji yang \"mendesak\" dan \"menarik\"\n",
        "        emoji_combinations = [\n",
        "            random.sample(spam_emojis, 2),\n",
        "            random.sample(spam_emojis, 3),\n",
        "            [random.choice(spam_emojis)] * 3,  # Repeat emoji\n",
        "        ]\n",
        "\n",
        "        for emojis in emoji_combinations:\n",
        "            # Tambah di awal\n",
        "            augmented_texts.append(''.join(emojis) + ' ' + text)\n",
        "            # Tambah di akhir\n",
        "            augmented_texts.append(text + ' ' + ''.join(emojis))\n",
        "            # Tambah di tengah (jika text cukup panjang)\n",
        "            if len(text.split()) > 5:\n",
        "                words = text.split()\n",
        "                mid = len(words) // 2\n",
        "                words.insert(mid, ''.join(emojis))\n",
        "                augmented_texts.append(' '.join(words))\n",
        "\n",
        "    else:  # ham\n",
        "        # Untuk ham, tambah emoji yang \"natural\" dan \"friendly\"\n",
        "        for emoji in random.sample(ham_emojis, 2):\n",
        "            augmented_texts.append(text + ' ' + emoji)\n",
        "            augmented_texts.append(emoji + ' ' + text)\n",
        "\n",
        "    return augmented_texts\n",
        "\n",
        "# Test data augmentation\n",
        "sample_texts = [\n",
        "    (\"Beli sekarang juga diskon besar\", \"spam\"),\n",
        "    (\"Selamat pagi semua\", \"ham\"),\n",
        "    (\"URGENT transfer uang sekarang\", \"spam\"),\n",
        "    (\"Terima kasih atas bantuan nya\", \"ham\")\n",
        "]\n",
        "\n",
        "print(\"üîÑ CONTOH DATA AUGMENTATION DENGAN EMOJI:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for original_text, label in sample_texts:\n",
        "    print(f\"\\nüìù Original ({label}): {original_text}\")\n",
        "    augmented = augment_text_with_emoji(original_text, label)\n",
        "\n",
        "    for i, aug_text in enumerate(augmented[:3], 1):  # Show first 3\n",
        "        print(f\"   {i}. {aug_text}\")\n",
        "\n",
        "print(f\"\\nüí° LANGKAH-LANGKAH IMPROVE EMOJI DETECTION:\")\n",
        "print(\"1. üìä Kumpulkan dataset spam/ham yang kaya emoji dari social media\")\n",
        "print(\"2. üîÑ Gunakan data augmentation seperti di atas\")\n",
        "print(\"3. üìö Fine-tune model dengan dataset yang sudah di-augment\")\n",
        "print(\"4. üéØ Test dan evaluate performa dengan emoji-rich test set\")\n",
        "print(\"5. üîß Adjust preprocessing jika perlu (normalize emoji, dll)\")\n",
        "\n",
        "print(f\"\\nüõ†Ô∏è ALTERNATIF LAIN:\")\n",
        "print(\"‚Ä¢ Gunakan model yang pre-trained khusus social media (seperti IndoBERTweet yang sudah kamu pakai)\")\n",
        "print(\"‚Ä¢ Tambah emoji embedding layer\")\n",
        "print(\"‚Ä¢ Preprocessing khusus: convert emoji ke text description\")\n",
        "print(\"‚Ä¢ Ensemble dengan rule-based emoji detector\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPLEMENTASI PRAKTIS - Steps untuk Emoji-Enhanced Training\n",
        "\n",
        "print(\"üöÄ STEP-BY-STEP GUIDE UNTUK EMOJI SUPPORT:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "üìã STEP 1: PREP DATASET DENGAN EMOJI\n",
        "=====================================\n",
        "1. Kumpulkan data spam/ham dari Twitter, Instagram, TikTok\n",
        "2. Pastikan ada banyak emoji dalam dataset\n",
        "3. Buat balanced dataset (50% ada emoji, 50% tanpa emoji)\n",
        "\n",
        "üìä STEP 2: DATA AUGMENTATION\n",
        "============================\n",
        "1. Jalankan script augmentation (seperti function di atas)\n",
        "2. Multiply dataset dengan emoji variations\n",
        "3. Ensure emoji patterns match realistic usage\n",
        "\n",
        "üß† STEP 3: TOKENIZER CHECK\n",
        "==========================\n",
        "1. ‚úÖ KAMU SUDAH PAKAI IndoBERTweet - ini BAGUS untuk emoji!\n",
        "2. Verify emoji tokenization dengan cell test di atas\n",
        "3. No need to retrain tokenizer, IndoBERTweet sudah optimal\n",
        "\n",
        "üéØ STEP 4: MODEL FINE-TUNING\n",
        "============================\n",
        "1. Load pre-trained model kamu\n",
        "2. Fine-tune dengan emoji-rich dataset\n",
        "3. Monitor performance di emoji vs non-emoji texts\n",
        "\n",
        "üìè STEP 5: EVALUATION & TESTING\n",
        "===============================\n",
        "1. Test dengan varied emoji combinations\n",
        "2. Check false positive/negative rates\n",
        "3. A/B test dengan model lama\n",
        "\n",
        "üîß STEP 6: DEPLOYMENT CONSIDERATIONS\n",
        "===================================\n",
        "1. Preprocessing pipeline harus handle emoji\n",
        "2. Monitor emoji pattern changes over time\n",
        "3. Regular updates dengan new emoji trends\n",
        "\"\"\")\n",
        "\n",
        "# Quick implementation example\n",
        "print(\"\\nüíª QUICK IMPLEMENTATION EXAMPLE:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "code_example = '''\n",
        "# 1. Load emoji-enhanced dataset\n",
        "df_emoji = pd.read_csv('spam_dataset_with_emoji.csv')\n",
        "\n",
        "# 2. Augment existing data\n",
        "augmented_data = []\n",
        "for idx, row in df.iterrows():\n",
        "    original = row['text']\n",
        "    label = row['label']\n",
        "\n",
        "    # Add original\n",
        "    augmented_data.append({'text': original, 'label': label})\n",
        "\n",
        "    # Add emoji variants\n",
        "    if random.random() < 0.3:  # 30% augmentation rate\n",
        "        variants = augment_text_with_emoji(original, label)\n",
        "        for variant in variants[:2]:  # Add 2 variants max\n",
        "            augmented_data.append({'text': variant, 'label': label})\n",
        "\n",
        "# 3. Fine-tune with emoji data\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=emoji_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    # ... other params\n",
        ")\n",
        "trainer.train()\n",
        "'''\n",
        "\n",
        "print(code_example)\n",
        "\n",
        "print(\"\\nüéØ HASIL YANG DIHARAPKAN:\")\n",
        "print(\"‚Ä¢ Model bisa detect emoji patterns dalam spam\")\n",
        "print(\"‚Ä¢ Better accuracy untuk social media content\")\n",
        "print(\"‚Ä¢ Robust terhadap emoji-heavy messages\")\n",
        "print(\"‚Ä¢ Maintain performance untuk text tanpa emoji\")\n",
        "\n",
        "print(f\"\\n‚úÖ KESIMPULAN:\")\n",
        "print(\"Tokenizer kamu (IndoBERTweet) SUDAH BAGUS untuk emoji!\")\n",
        "print(\"Yang perlu: FINE-TUNE model dengan dataset yang kaya emoji.\")\n",
        "print(\"Gak perlu train tokenizer dari awal! üéâ\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
